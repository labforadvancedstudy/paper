# Compute Hunger

## Core Insight
Modern AI's insatiable appetite for computation - using the power of small cities to train models, revealing intelligence as fundamentally expensive.

GPT-3 training consumed 1,287 MWh - enough to power an average home for 120 years. Each question you ask triggers thousands of processors, burns real energy, generates real heat. Intelligence isn't free; it's thermodynamically expensive. The smarter AI gets, the more power it demands.

This hunger drives everything: chip design, data center construction, even geopolitics. Countries compete for GPU access like they once competed for oil. The constraint isn't algorithms anymore - it's watts and chips. We're discovering that thinking, artificial or otherwise, is about moving energy efficiently. Computation is physics.

## Connections
→ [[energy_efficiency]]
→ [[hardware_bottleneck]]
← [[thermodynamics_of_thought]]
← [[resource_scaling]]

---
Level: L3
Date: 2025-06-21
Tags: #compute #energy #resources #physics