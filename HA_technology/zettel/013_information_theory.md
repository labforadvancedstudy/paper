# Information Theory

## Core Insight
Information is surprise mathematized - the less probable the message, the more information it carries, revealing that meaning itself has mathematical structure.

Claude Shannon's revelation: information isn't about meaning but about possibility space. A coin flip carries 1 bit - it cuts possibility in half. A dice roll carries 2.58 bits - it selects from 6 options. Information measures selection from chaos.

The profound shift: from semantic to syntactic. Information theory doesn't care if you send Shakespeare or gibberish - only how much choice was involved. This let us engineer meaning, compress knowledge, error-correct reality.

Every digital system incarnates information theory. Compression finds patterns to reduce surprise. Encryption maximizes surprise for eavesdroppers. Error correction adds controlled redundancy. We've learned to sculpt with entropy itself.

## Connections
→ [[010_signal_transmission]]
→ [[041_information_substrate]]
← [[002_screen_glowing]]
← [[022_distributed_systems]]

---
Level: L2
Date: 2025-06-21
Tags: #information #mathematics #communication #entropy