# Neural Networks
## Core Insight
Neural networks don't copy brains - they discover that weighted connections and non-linear transformations are universal approximators.

## The Beautiful Simplicity

Basic unit (neuron):
```
Inputs × Weights → Sum → Activation → Output
```

Stack millions. Add feedback. Intelligence emerges.

## Why They Work

**Universal Approximation**: Can learn any function
**Hierarchical Features**: Edges → Shapes → Objects → Concepts
**Distributed Representation**: Knowledge spread across network
**Graceful Degradation**: Damage doesn't destroy
**Generalization**: Learn patterns, not memorize

Nature found this. We rediscovered.

## The Architecture Zoo

**Feedforward**: Information flows forward
**Convolutional**: Spatial patterns (vision)
**Recurrent**: Temporal patterns (sequences)
**Transformer**: Attention mechanisms
**Generative**: Create new data

Each architecture: Different inductive bias.

## Training Magic

**Backpropagation**: Error flows backward
**Gradient Descent**: Roll downhill to solution
**Stochastic**: Random samples guide
**Batch Processing**: Learn from groups
**Regularization**: Prevent overfitting

Simple rules. Complex behavior.

## The Mystery

We know:
- How to build them
- How to train them
- What they do

We don't know:
- Why they work so well
- What they really learn
- How they represent knowledge

Black boxes that work.

## Connections
→ [[006_deep_learning]]
→ [[007_backpropagation]]
→ [[008_representation_learning]]
← [[001_ai_essence]]

---
Level: L2
Date: 2025-06-21
Tags: #neural_networks #deep_learning #architecture #learning