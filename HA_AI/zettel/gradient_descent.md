# Gradient Descent

## Core Insight
Like a blind hiker finding the valley by always stepping downhill, AI learns by falling toward correctness one tiny step at a time.

Wrong answer. By how much? The error spreads backward through the network like ripples in reverse. Each weight asks: "How much did I contribute to this mistake?" Then nudges itself in the direction of less wrong. Millions of tiny adjustments, each insignificant alone, together reshape the network's understanding.

The mathematical elegance hides brutal simplicity: make mistakes, measure mistakes, reduce mistakes, repeat. No insight, no eureka moments, just relentless incremental improvement. Yet from this mechanical process emerges the ability to recognize faces, translate languages, play games no human can win. Intelligence through optimization.

## Connections
→ [[backpropagation]]
→ [[optimization_landscape]]
← [[error_correction]]
← [[learning_rate]]

---
Level: L2
Date: 2025-06-21
Tags: #optimization #learning #mathematics #emergence