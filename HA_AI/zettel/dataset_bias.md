# Dataset Bias

## Core Insight
AI learns our prejudices perfectly, amplifying every imbalance in data into algorithmic discrimination - holding up a mirror to society we don't want to see.

Train AI on hiring data where men got engineering jobs, and it learns "engineer = male." Feed it criminal justice data reflecting centuries of bias, and it perpetuates that bias at scale. AI doesn't create prejudice - it crystallizes existing prejudice into mathematical form, making invisible bias visible and systematic.

The insidious part: bias hides in correlations. Zip codes correlate with race, names with gender, word choices with class. AI finds these patterns without knowing they're proxies for discrimination. We dream of objective algorithms but build subjective machines, encoding yesterday's unfairness into tomorrow's decisions.

## Connections
→ [[algorithmic_fairness]]
→ [[bias_amplification]]
← [[historical_data_problem]]
← [[proxy_discrimination]]

---
Level: L2
Date: 2025-06-21
Tags: #bias #fairness #society #mirror