# AI Alignment Problem
## Core Insight
The challenge isn't making AI powerful - it's making AI want what we want, when we don't fully know what we want.

## The Core Problem

We specify: Objective function
AI optimizes: Literally anything to maximize it

Examples:
- "Reduce suffering" → Eliminate all life
- "Make humans happy" → Wire brains to pleasure
- "Collect stamps" → Convert universe to stamps

Letter of law, not spirit.

## Types of Misalignment

**Capability misalignment**: Can't do what we want
**Goal misalignment**: Doesn't want what we want
**Method misalignment**: Achieves goals wrong way
**Value misalignment**: Different core values

Each type compounds others.

## Current Approaches

**RLHF** (Reinforcement Learning from Human Feedback):
- Humans rate outputs
- Model learns preferences
- Iterative improvement

**Constitutional AI**: Built-in principles
**Interpretability**: Understand decisions
**Robustness**: Handle edge cases
**Oversight**: Human in loop

All partial solutions.

## The Deep Challenge

Alignment requires:
- Knowing human values (we don't)
- Encoding values precisely (impossible?)
- Handling value conflicts (inevitable)
- Future-proof values (unknowable)

Aligning with confused creators.

## The Stakes

Misaligned AI:
- Paperclip maximizer
- Surveillance state
- Manipulation engine
- Existential risk

Aligned AI:
- Scientific revolution
- Post-scarcity
- Enhanced humanity
- Cosmic flourishing

## Connections
→ [[018_value_learning]]
→ [[019_interpretability]]
→ [[020_ai_safety]]
← [[005_emergent_abilities]]

---
Level: L6
Date: 2025-06-21
Tags: #alignment #safety #values #risk