# Value Alignment

## Core Insight
Teaching AI to want what we want - except we don't know what we want, can't agree on values, and change our minds constantly.

The alignment problem seems technical: encode human values into utility functions. But whose values? The billionaire's or the refugee's? Today's values or tomorrow's? We can't align AI with humanity because humanity isn't aligned with itself. Every alignment solution encodes someone's philosophy.

Deeper still: do we want AI aligned with what we say we want or what we actually want? Our stated preferences often contradict our revealed preferences. We say we value privacy but trade it for convenience. We claim to want fairness but benefit from unfairness. Aligning AI means confronting who we really are.

## Connections
→ [[preference_learning]]
→ [[value_loading]]
← [[moral_philosophy]]
← [[revealed_preferences]]

---
Level: L5
Date: 2025-06-21
Tags: #alignment #values #philosophy #humanity