# Interpretability Illusion

## Core Insight
We build tools to understand AI decisions, but the explanations might be stories we tell ourselves - comforting narratives that hide deeper mysteries.

Saliency maps highlight which pixels influenced a decision. Feature visualization shows what neurons detect. Attention weights reveal focus patterns. But do these explain thinking or just show correlations? When we visualize a neuron firing for curves, do we understand why it cares about curves?

The illusion is thinking mechanical transparency equals understanding. We can trace every computation in a calculator without understanding arithmetic. We map every neuron's activation without grasping intelligence. Interpretability tools might be building elaborate mirrors that reflect our assumptions back at us.

## Connections
→ [[explanation_gap]]
→ [[mechanistic_understanding]]
← [[correlation_causation]]
← [[narrative_fallacy]]

---
Level: L5
Date: 2025-06-21
Tags: #interpretability #understanding #illusion #explanation